{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 367
    },
    "colab_type": "code",
    "id": "Q1wnryKmxkOz",
    "outputId": "f719230f-6b4c-4861-e404-884ecc7d57ec"
   },
   "outputs": [],
   "source": [
    "#import sys\n",
    "#Install packages prefer conda if you are using anaconda otherwise try with pip, uncomment only if install is needed (if you use Google colab pip should work)\n",
    "\n",
    "#conda:\n",
    "#os.system('conda install GPy -c conda-forge')\n",
    "#os.system('conda install GPyOpt -c conda-forge')\n",
    "\n",
    "#pip:\n",
    "#!{sys.executable} -m pip install GPy\n",
    "#!{sys.executable} -m pip install GPyOpt\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import GPyOpt\n",
    "from torchvision import datasets, transforms, utils\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import ParameterSampler, RandomizedSearchCV, cross_val_score\n",
    "from scipy.stats import uniform\n",
    "import random\n",
    "\n",
    "np.random.seed(32)\n",
    "random.seed(32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G6ZbPxoDxkPB"
   },
   "source": [
    "# Hyperparameters tuning using Bayesian Optimization with the GPyOpt library\n",
    "\n",
    "As you are going to see, the project for this first part of the course will be to use Bayesian Optimization to tune the hyperparameters of a Deep Neural Network. To do that you are going to use a library that is more optimized compare to the code from scratch that we have written before. We are going to use [GPyOPt](https://sheffieldml.github.io/GPyOpt/) developed by the Machine Learning group of the University of Sheffield and it is based on GPy, which is a framework for using Gaussian Process in Python.\n",
    "\n",
    "In this exercise you are going to use Bayesian Optimization to select the best hyperparameters of a random forest trained on part of the MNIST dataset. You are going also to compare it with respect to a random search on the hyperparameter space.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uup6huN4xkPJ"
   },
   "outputs": [],
   "source": [
    "def load_MNIST():\n",
    "    '''\n",
    "    Function to load the MNIST training and test set with corresponding labels.\n",
    "\n",
    "    :return: training_examples, training_labels, test_examples, test_labels\n",
    "    '''\n",
    "\n",
    "    # we want to flat the examples\n",
    "\n",
    "    training_set = datasets.MNIST(root='./data', train=True, download=True, transform= None)\n",
    "    test_set = datasets.MNIST(root='./data', train=False, download=True, transform= None)\n",
    "\n",
    "    Xtrain = training_set.data.numpy().reshape(-1,28*28)\n",
    "    Xtest = test_set.data.numpy().reshape(-1,28*28)\n",
    "\n",
    "    ytrain = training_set.targets.numpy()\n",
    "    ytest = test_set.targets.numpy()\n",
    "\n",
    "    return Xtrain, ytrain, Xtest, ytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CIx5w0wjxkPR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Information about the new datasets\n",
      "Training set shape: (60000, 784)\n",
      "Test set shape (10000, 784)\n"
     ]
    }
   ],
   "source": [
    "## since training a random forest on the entire dataset takes some time\n",
    "## we can consider only few labels, like 3, 5, 8 and 9\n",
    "\n",
    "## we can load the training set and test set\n",
    "Xtrain, ytrain, Xtest, ytest = load_MNIST()\n",
    "\n",
    "## we use a mask to selects those subsets\n",
    "#train_filter = np.isin(ytrain, [3, 5, 8, 9])\n",
    "#test_filter = np.isin(ytest, [3, 5, 8, 9])\n",
    "\n",
    "# apply the mask to the entire dataset\n",
    "#Xtrain, ytrain = Xtrain[train_filter], ytrain[train_filter]\n",
    "#Xtest, ytest = Xtest[test_filter], ytest[test_filter]\n",
    "\n",
    "# print some information\n",
    "print('Information about the new datasets')\n",
    "print('Training set shape:', Xtrain.shape)\n",
    "print('Test set shape', Xtest.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YLelVc36xkPa"
   },
   "source": [
    "## Random forest\n",
    "\n",
    "As you have seen in your Machine Learning course, there are a lot of hyperparameters to choose before training a Random Forest. If you look at the [sklearn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) you can notice all the paramters. For this exercise we will focus only on the following four hyperparameters:\n",
    "\n",
    "1. **n_estimators**: the number of decision trees that are in the forest;\n",
    "2. **criterion**: the criterion to evaluate the split. We should decide between *Gini impurity* (`gini`) and *information gain* (`entropy`);\n",
    "3. **max_depth**: Maximum depth of the trees. If None, the tree expands until we have a element in all the leaves or  until all leaves contain less than min_samples_split samples.\n",
    "4. **max_features**: The number of features to consider when looking for the best split. We have to choose among `'sqrt'` where we consider `sqrt(n_features)`, `'log2'` where we consider `log2(n_features)`. It is possible to use `None` where we consider all the features, but it makes everything super slow, so we are avoiding it.\n",
    "\n",
    "Remember that when we are using Random Forest we can avoid running the cross-validation to get the validation error as approximation of the test error, but instead we can use the *out of bag* error to get an approximation of the test error we are going to get when we consider unseen example. \n",
    "\n",
    "<font color='blue'> Tasks:\n",
    "1. You should define a `RandomForestClassifier` with default parameters, and compute the *out_of_bag* error (you should use `oob_score=True` when you define the classifier) and then the test error. This would be the baseline for the experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gKz2Nq5OxkPc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default model out of bag error 0.8776666666666667\n",
      "Default model test accuracy 0.9458\n"
     ]
    }
   ],
   "source": [
    "model = RandomForestClassifier(oob_score = True)\n",
    "model.fit(Xtrain, ytrain)\n",
    "print('Default model out of bag error', model.oob_score_)\n",
    "print('Default model test accuracy', model.score(Xtest, ytest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D2DIwTl5xkPk"
   },
   "source": [
    "### Random search \n",
    "\n",
    "A possible way to find the best hyperparameters is to use random search over the parameter space. To perform this operation we can use `RandomizedSearchCV`. You can look at the documentation [here](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html). However, since we do not need cross-validation for Random Forests, we can use `ParameterSampler` to sample random parameters from our parameter space. Look at the documentation [here](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.ParameterSampler.html#sklearn.model_selection.ParameterSampler) To be able to use this method you should first define a dictionary of the hyperparameters you want to optimize, in this case the four we mentioned above, and then decide a way to evaluate the model and how many folds for the cross-validation. The dictionary is of the form:\n",
    "\n",
    "```python\n",
    "params = {\"name_params\": uniform(0, 1), # if it is continuous between 0 and 1\n",
    "           \"name_params2\": range(1,50), # if it is discrete but integer\n",
    "           \"name_params3\": ['name1', 'name2']} # if it is discrete but string`\n",
    "```\n",
    "\n",
    "<font color='blue'> Tasks:\n",
    "\n",
    "1. <font color='blue'> Crete the dictionary of the hyperparameters, we recommend keeping the value of the number of trees less than 100 otherwise it takes a very long time. You can try to increase it at home, but it will likely not be possible in the exercise session as you would be stuck for some time. After that, you can use the dictionary to create a random hyperparameter using `ParameterSampler`. The `ParameterSampler` call is already provided.\n",
    "\n",
    "2. <font color='blue'> Using the parameters returned by the `ParameterSampler` you should fit a random forest, and for each iteration you should store the best value of the `model.oob_score_` you get. **HINT:** You can access the hyperparameters value by the name you used in the defined dictionary. (The whole process would take like 5/10 mins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tQP9z8AOxkPm"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Param list\n",
      "[{'n_estimators': 76, 'max_features': 'log2', 'max_depth': 40, 'criterion': 'entropy'}, {'n_estimators': 80, 'max_features': 'log2', 'max_depth': 80, 'criterion': 'gini'}, {'n_estimators': 74, 'max_features': 'log2', 'max_depth': 100, 'criterion': 'entropy'}, {'n_estimators': 35, 'max_features': 'log2', 'max_depth': 70, 'criterion': 'gini'}, {'n_estimators': 41, 'max_features': 'log2', 'max_depth': 50, 'criterion': 'entropy'}, {'n_estimators': 37, 'max_features': 'log2', 'max_depth': 10, 'criterion': 'entropy'}, {'n_estimators': 76, 'max_features': 'sqrt', 'max_depth': 80, 'criterion': 'entropy'}, {'n_estimators': 53, 'max_features': 'sqrt', 'max_depth': 20, 'criterion': 'gini'}, {'n_estimators': 44, 'max_features': 'log2', 'max_depth': 60, 'criterion': 'entropy'}, {'n_estimators': 11, 'max_features': 'log2', 'max_depth': 30, 'criterion': 'gini'}, {'n_estimators': 50, 'max_features': 'sqrt', 'max_depth': 40, 'criterion': 'entropy'}, {'n_estimators': 44, 'max_features': 'sqrt', 'max_depth': 100, 'criterion': 'entropy'}, {'n_estimators': 98, 'max_features': 'log2', 'max_depth': 50, 'criterion': 'gini'}, {'n_estimators': 59, 'max_features': 'sqrt', 'max_depth': 70, 'criterion': 'gini'}, {'n_estimators': 10, 'max_features': 'sqrt', 'max_depth': 10, 'criterion': 'gini'}, {'n_estimators': 29, 'max_features': 'sqrt', 'max_depth': 60, 'criterion': 'gini'}, {'n_estimators': 24, 'max_features': 'log2', 'max_depth': 90, 'criterion': 'entropy'}, {'n_estimators': 14, 'max_features': 'log2', 'max_depth': 30, 'criterion': 'entropy'}, {'n_estimators': 84, 'max_features': 'sqrt', 'max_depth': 70, 'criterion': 'gini'}, {'n_estimators': 26, 'max_features': 'sqrt', 'max_depth': 40, 'criterion': 'entropy'}]\n",
      "0\n",
      "{'n_estimators': 76, 'max_features': 'log2', 'max_depth': 40, 'criterion': 'entropy'}\n",
      "OOB found: 0.9585166666666667\n",
      "It took 34.15441679954529 seconds\n",
      "1\n",
      "{'n_estimators': 80, 'max_features': 'log2', 'max_depth': 80, 'criterion': 'gini'}\n",
      "OOB found: 0.9585333333333333\n",
      "It took 24.434396266937256 seconds\n",
      "2\n",
      "{'n_estimators': 74, 'max_features': 'log2', 'max_depth': 100, 'criterion': 'entropy'}\n",
      "OOB found: 0.9576333333333333\n",
      "It took 28.371780157089233 seconds\n",
      "3\n",
      "{'n_estimators': 35, 'max_features': 'log2', 'max_depth': 70, 'criterion': 'gini'}\n",
      "OOB found: 0.9419166666666666\n",
      "It took 12.42845106124878 seconds\n",
      "4\n",
      "{'n_estimators': 41, 'max_features': 'log2', 'max_depth': 50, 'criterion': 'entropy'}\n",
      "OOB found: 0.9476\n",
      "It took 17.035150051116943 seconds\n",
      "5\n",
      "{'n_estimators': 37, 'max_features': 'log2', 'max_depth': 10, 'criterion': 'entropy'}\n",
      "OOB found: 0.9177166666666666\n",
      "It took 11.303446054458618 seconds\n",
      "6\n",
      "{'n_estimators': 76, 'max_features': 'sqrt', 'max_depth': 80, 'criterion': 'entropy'}\n",
      "OOB found: 0.96325\n",
      "It took 76.43375420570374 seconds\n",
      "7\n",
      "{'n_estimators': 53, 'max_features': 'sqrt', 'max_depth': 20, 'criterion': 'gini'}\n",
      "OOB found: 0.9589333333333333\n",
      "It took 31.000745058059692 seconds\n",
      "8\n",
      "{'n_estimators': 44, 'max_features': 'log2', 'max_depth': 60, 'criterion': 'entropy'}\n",
      "OOB found: 0.9507333333333333\n",
      "It took 13.215766191482544 seconds\n",
      "9\n",
      "{'n_estimators': 11, 'max_features': 'log2', 'max_depth': 30, 'criterion': 'gini'}\n",
      "OOB found: 0.8623833333333333\n",
      "It took 2.9104959964752197 seconds\n",
      "10\n",
      "{'n_estimators': 50, 'max_features': 'sqrt', 'max_depth': 40, 'criterion': 'entropy'}\n",
      "OOB found: 0.9587166666666667\n",
      "It took 33.51989507675171 seconds\n",
      "11\n",
      "{'n_estimators': 44, 'max_features': 'sqrt', 'max_depth': 100, 'criterion': 'entropy'}\n",
      "OOB found: 0.9571666666666667\n",
      "It took 29.91184091567993 seconds\n",
      "12\n",
      "{'n_estimators': 98, 'max_features': 'log2', 'max_depth': 50, 'criterion': 'gini'}\n",
      "OOB found: 0.9612333333333334\n",
      "It took 26.717307806015015 seconds\n",
      "13\n",
      "{'n_estimators': 59, 'max_features': 'sqrt', 'max_depth': 70, 'criterion': 'gini'}\n",
      "OOB found: 0.9609166666666666\n",
      "It took 40.33083987236023 seconds\n",
      "14\n",
      "{'n_estimators': 10, 'max_features': 'sqrt', 'max_depth': 10, 'criterion': 'gini'}\n",
      "OOB found: 0.8686666666666667\n",
      "It took 4.962534189224243 seconds\n",
      "15\n",
      "{'n_estimators': 29, 'max_features': 'sqrt', 'max_depth': 60, 'criterion': 'gini'}\n",
      "OOB found: 0.9470666666666666\n",
      "It took 23.701986074447632 seconds\n",
      "16\n",
      "{'n_estimators': 24, 'max_features': 'log2', 'max_depth': 90, 'criterion': 'entropy'}\n",
      "OOB found: 0.9298\n",
      "It took 9.07854413986206 seconds\n",
      "17\n",
      "{'n_estimators': 14, 'max_features': 'log2', 'max_depth': 30, 'criterion': 'entropy'}\n",
      "OOB found: 0.8903666666666666\n",
      "It took 5.461904764175415 seconds\n",
      "18\n",
      "{'n_estimators': 84, 'max_features': 'sqrt', 'max_depth': 70, 'criterion': 'gini'}\n",
      "OOB found: 0.9639666666666666\n",
      "It took 66.05904006958008 seconds\n",
      "19\n",
      "{'n_estimators': 26, 'max_features': 'sqrt', 'max_depth': 40, 'criterion': 'entropy'}\n",
      "OOB found: 0.946\n",
      "It took 19.766479015350342 seconds\n"
     ]
    }
   ],
   "source": [
    "import time \n",
    "\n",
    "# hyperparams dictionary \n",
    "domain = {\"criterion\": ['gini', 'entropy'],\n",
    "          \"n_estimators\": np.arange(1,101,1),\n",
    "           \"max_depth\": np.arange(10,110,10), \n",
    "           \"max_features\": ['sqrt', 'log2']}\n",
    "\n",
    "#rs = RandomizedSearchCV(model, param_distributions=domain, cv=3, verbose =2, n_iter=10)\n",
    "#rs.fit(Xtrain, ytrain)\n",
    "\n",
    "# create the ParameterSampler\n",
    "param_list = list(ParameterSampler(domain, n_iter=20, random_state=32))\n",
    "print('Param list')\n",
    "print(param_list)\n",
    "#rounded_list = [dict((k,v) for (k, v) in d.items()) for d in param_list]\n",
    "\n",
    "#print('Random parameters we are going to consider')\n",
    "#print(rounded_list)\n",
    "\n",
    "## now we can train the random forest using these parameters tuple, and for\n",
    "## each iteration we store the best value of the oob\n",
    "\n",
    "current_best_oob = 0\n",
    "iteration_best_oob = 0 \n",
    "max_oob_per_iteration = []\n",
    "i = 0\n",
    "for params in param_list:\n",
    "    print(i)\n",
    "    print(params)\n",
    "    n, c, d, f = params[\"n_estimators\"], params[\"criterion\"], params[\"max_depth\"], params[\"max_features\"]\n",
    "    model = RandomForestClassifier(n_estimators=n, criterion=c, max_depth=d, max_features=f, oob_score=True)\n",
    "    \n",
    "    start = time.time()\n",
    "    model.fit(Xtrain, ytrain)\n",
    "    end = time.time()\n",
    "    model_oob = model.oob_score_\n",
    "    print('OOB found:', model_oob)\n",
    "    if model_oob > current_best_oob:\n",
    "        current_best_oob = model_oob\n",
    "        iteration_best_oob = i\n",
    "    \n",
    "    max_oob_per_iteration.append(current_best_oob)\n",
    "    i += 1\n",
    "    print(f'It took {end - start} seconds')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4NmwnVJ2xkPt"
   },
   "source": [
    "### Bayesian Optimization\n",
    "\n",
    "The procedure we are interested in, instead, is Bayesian Optimization. As before, also for GPyOpt, we should start by defining the set of hyperparameters you want to optimize. If some hyperparameters are discrete and represented by a string, you should integer instead and change them when you are initializing the classifier. The dictionary should have this form `{'name': 'name_of_params', 'type': 'type_of_variable', 'domain': domain_given_as_tuple}`. Note that the domain should specify as tuple. An example is given by\n",
    "\n",
    "```python\n",
    "params = [{'name': 'var_1', 'type': 'continuous', 'domain': (0,10)},\n",
    "            {'name': 'var_2', 'type': 'discrete', 'domain': (0,5,10)},\n",
    "            {'name': 'var_3', 'type': 'categorical', 'domain': (0,1)}]`\n",
    "```\n",
    "\n",
    "In the example above, suppose that `'var_3'` is usually used as string, where `0 = 'l1'` and `1 = 'l2'`, for example.\n",
    "If you want to define a large discrete interval, you should define the tuple as `tuple(np.arange(1,100,1, dtype= np.int))`. \n",
    "\n",
    "The second ingredient we should define is the objective function we want to maximize. Since GPyOpt minimize things, when you are returning the value of the objective you return it negated. This function, usually defined as `objective_function(x)`, takes a general parameter `x`. This represents all the parameter we need. Therefore, when we define the function, we should collect the parameters by doing `param = x[0]`. Then we are able to pass the parameters to the classifier. In the example above we will have that `param[0]` is the 'var_1', `param[0]` is 'var_2', whereas, since 'var_3' is usually a string we should get back the string value, therefore `if param[2] == 0: var_3 = l1 else: var_3 = 'l2'`.\n",
    "Note that by default GPyOpt initializes with 5 points and considers Bayesian optimization after this, therefore max_iter=15 will give you a total of 20 function evaluation. \n",
    "In a general way, the scheme for the objective function is similar to:\n",
    "\n",
    "```python\n",
    "def objective_function(x):\n",
    "\n",
    "    param = x[0]\n",
    "    \n",
    "    if param[2] == 1:\n",
    "        val_3 = 'l1'\n",
    "    else:\n",
    "        val_3 = 'l2'\n",
    "        \n",
    "    model = your_model(param[0],param[1],val_3)\n",
    "    model.fit(X,y)\n",
    "    \n",
    "    ## then you will compute the measure you want to maximize\n",
    "    ## usually you should consider a validation set\n",
    "    ## in case of random forest you return the - oob score\n",
    "    return - model.score(Xvalid, yvalid)\n",
    "```\n",
    "\n",
    "In case of random forest, you should return the `oob_score` and not the validation score. If you are going to use other models you should consider the validation set, instead.\n",
    "\n",
    "When you have these two ingredients, you should create an Bayesian Optimization instancer using the following function `GPyOpt.methods.BayesianOptimization`, see the complete description [here](https://gpyopt.readthedocs.io/en/latest/GPyOpt.methods.html). Run it calling the `run_optimization` function of this instance, documentation [here](https://gpyopt.readthedocs.io/en/latest/GPyOpt.core.html).\n",
    "\n",
    "At the end of the optimization, you can access the array of the best parameters by `x_best = opt.X[np.argmin(opt.Y)]`. You can also collect the best `oob_score` per iteration using \n",
    "`y_bo = np.maximum.accumulate(-opt.Y).ravel()`.\n",
    "\n",
    "\n",
    "<font color='blue'> Tasks:\n",
    "\n",
    "1. <font color='blue'>You should define all the ingredients needed for GPyOpt: 1) define the dictionary of hyperparameters, following the GPyOpt rules; 2) define the objective function you want to optimize. GPyOpt usually minimize (there is a parameter called maximize but it appears to have no effect), therefore, since in your setting you want to maximize the oob_score, you should return the negative oob_score; 3) run the optimization. When you create the `GPyOpt.methods.BayesianOptimization` instance you should define the following parameters: `f`,`domain`, `acquisition_type`, `exploration_weight`. The documentation for the GPyOpt package is quite limited and it is quite unclear to define how to define certain parameter, so elaborate hints are given here. The fitting procedure should take $\\sim 10$ minutes. Do not worry about the warning \"The set cost function is ignored! LCB acquisition does not make sense with cost.\" which might occur when using LCB. From discussions on github it appears to be a known error and might be fixed in the lastest git master branch.\n",
    "\n",
    "2. <font color='blue'>In the same plot, show the best `oob_score` per iteration you obtain using the random search and the bayesian optimization. What do you see?\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o2TTDArnxkPv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[46. 50.  0.  0.]]\n",
      "It took 13.857320070266724 seconds\n",
      "0.9495333333333333\n",
      "[[99. 30.  1.  1.]]\n",
      "It took 75.05217099189758 seconds\n",
      "0.96485\n",
      "[[61. 50.  1.  1.]]\n",
      "It took 48.174713134765625 seconds\n",
      "0.9613\n",
      "[[21. 40.  0.  0.]]\n",
      "It took 8.640379190444946 seconds\n",
      "0.9218166666666666\n",
      "[[ 2. 90.  1.  1.]]\n",
      "It took 2.035022258758545 seconds\n",
      "0.5368166666666667\n",
      "[[51. 10.  0.  0.]]\n",
      "It took 11.602973222732544 seconds\n",
      "0.9192333333333333\n",
      "[[100.  80.   0.   0.]]\n",
      "It took 27.8919038772583 seconds\n",
      "0.9606333333333333\n",
      "[[100.  50.   0.   0.]]\n",
      "It took 25.33799695968628 seconds\n",
      "0.9605166666666667\n",
      "[[ 1. 10.  0.  0.]]\n",
      "It took 0.47827577590942383 seconds\n",
      "0.32365\n",
      "[[73. 30.  0.  0.]]\n",
      "It took 18.568702936172485 seconds\n",
      "0.9583333333333334\n",
      "[[ 78. 100.   1.   1.]]\n",
      "It took 58.71469211578369 seconds\n",
      "0.9636\n",
      "[[100. 100.   1.   1.]]\n",
      "It took 90.5682430267334 seconds\n",
      "0.9658166666666667\n",
      "[[100.  10.   0.   0.]]\n",
      "It took 21.56133723258972 seconds\n",
      "0.9300666666666667\n",
      "[[45. 30.  1.  1.]]\n",
      "It took 42.85395097732544 seconds\n",
      "0.95705\n",
      "[[80. 70.  1.  1.]]\n",
      "It took 57.816468238830566 seconds\n",
      "0.9641333333333333\n",
      "[[88. 90.  1.  1.]]\n",
      "It took 60.61656403541565 seconds\n",
      "0.9641666666666666\n",
      "[[74. 10.  1.  1.]]\n",
      "It took 39.431727170944214 seconds\n",
      "0.9417833333333333\n",
      "[[83. 50.  1.  1.]]\n",
      "It took 54.67972707748413 seconds\n",
      "0.9636666666666667\n",
      "[[ 89. 100.   0.   0.]]\n",
      "It took 22.66503596305847 seconds\n",
      "0.9599333333333333\n",
      "[[100.  70.   1.   1.]]\n",
      "It took 68.3013048171997 seconds\n",
      "0.9652666666666667\n",
      "The best parameters obtained: n_estimators=100.0, max_depth=100.0, max_features=1.0, criterion=1.0\n"
     ]
    }
   ],
   "source": [
    "## define the domain of the considered parameters\n",
    "n_estimators = tuple(np.arange(1,101,1, dtype= np.int))\n",
    "# print(n_estimators)\n",
    "max_depth = tuple(np.arange(10,110,10, dtype= np.int))\n",
    "# max_features = ('log2', 'sqrt', None)\n",
    "max_features = (0, 1)\n",
    "# criterion = ('gini', 'entropy')\n",
    "criterion = (0, 1)\n",
    "\n",
    "\n",
    "# define the dictionary for GPyOpt\n",
    "domain = [{'name': 'n_estimators', 'type': 'discrete', 'domain': n_estimators},\n",
    "            {'name': 'max_depth', 'type': 'discrete', 'domain': max_depth},\n",
    "            {'name': 'max_features', 'type': 'categorical', 'domain': max_features},\n",
    "            {'name': 'criterion', 'type': 'categorical', 'domain': criterion}]\n",
    "\n",
    "\n",
    "## we have to define the function we want to maximize --> validation accuracy, \n",
    "## note it should take a 2D ndarray but it is ok that it assumes only one point\n",
    "## in this setting\n",
    "def objective_function(x): \n",
    "    print(x)\n",
    "    # we have to handle the categorical variables that is convert 0/1 to labels\n",
    "    # log2/sqrt and gini/entropy\n",
    "    \n",
    "    param = x[0]\n",
    "\n",
    "    if param[2] == 1:\n",
    "        max_f = 'sqrt'\n",
    "    else:\n",
    "        max_f = 'log2'\n",
    "    \n",
    "    if param[3] == 1:\n",
    "        crit = 'entropy'\n",
    "    else:\n",
    "        crit = 'gini'\n",
    "   \n",
    "    start = time.time()\n",
    "    #fit the model\n",
    "    model = RandomForestClassifier(n_estimators=int(param[0]), criterion=crit, max_depth=int(param[1]), max_features=max_f, oob_score=True)\n",
    "    model.fit(Xtrain, ytrain)\n",
    "    end = time.time()\n",
    "    print(f'It took {end - start} seconds')\n",
    "    \n",
    "    print(model.oob_score_)\n",
    "    return - model.oob_score_\n",
    "\n",
    "\n",
    "opt = GPyOpt.methods.BayesianOptimization(f = objective_function,   # function to optimize\n",
    "                                              domain = domain,         # box-constrains of the problem\n",
    "                                              acquisition_type = 'EI'      # Select acquisition function MPI, EI, LCB\n",
    "                                             )\n",
    "opt.acquisition.exploration_weight=.1\n",
    "\n",
    "opt.run_optimization(max_iter = 15) \n",
    "\n",
    "x_best = opt.X[np.argmin(opt.Y)]\n",
    "print(\"The best parameters obtained: n_estimators=\" + str(x_best[0]) + \", max_depth=\" + str(x_best[1]) + \", max_features=\" + str(\n",
    "    x_best[2])  + \", criterion=\" + str(\n",
    "    x_best[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1DvhNlzcxkP2"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "## comparison between random search and bayesian optimization\n",
    "## we can plot the maximum oob per iteration of the sequence\n",
    "\n",
    "# collect the maximum each iteration of BO, note that it is also provided by GPOpt in Y_Best\n",
    "y_bo = np.maximum.accumulate(-opt.Y).ravel()\n",
    "# define iteration number\n",
    "xs = np.arange(0, len(max_oob_per_iteration))\n",
    "\n",
    "plt.plot(xs, max_oob_per_iteration, 'o-', color = 'red', label='Random Search')\n",
    "plt.plot(xs, y_bo, 'o-', color = 'blue', label='Bayesian Optimization')\n",
    "plt.legend()\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Out of bag error')\n",
    "plt.title('Comparison between Random Search and Bayesian Optimization')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___NOTES:___\n",
    "\n",
    "If we choose same parameter-space (I guess you can call it that), we can compare the two approaches. One will see that the Bayesian Optimization is quite steady with a high oob-score from the start. The approach using random sampling of the parameters will probably differ due to the randomness that defines it. Eventually if the tests, T, are many enough, they will probably tend toward the same score. Bayesian Optimization is therefore preferable since it bases its choice of parameters on probabilities and the distribution of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g6dHDP0rU-5r"
   },
   "source": [
    "# [OPTIONAL] Investigate the acquisition function (not part of the exam)\n",
    "<br>\n",
    "<font color='blue'>\n",
    "1. <font color='blue'> Investigate the dimensionality of the GP (look at the model subclass), how many dimensions are there? Why?\n",
    " \n",
    "2.<font color='blue'> Take a look at the acqusition function, as this is more than 2 dimensional in this case it is not trivial to plot it, therefore keep the categorical variables fixed at some value that you decide and evaluate the acquisition function on a grid for the remaining two variables. The acquisition function can be evaluated using opt.acquisition.acquisition_function and you can create the grid using np.meshgrid. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tQ666LjBQZPG"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "colab": {
   "collapsed_sections": [],
   "name": "Lecture_2_Exercise2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
