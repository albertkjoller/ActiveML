{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wKx8dXanZSDP"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# we set a seed to make the results reproducible\n",
    "np.random.seed(32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GAn8tzTFVm0Z",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "# Gaussian Processes - Exercise Session 1\n",
    "\n",
    "## Relationship between linear models and conditional Gaussian distribution\n",
    "\n",
    "In this exercise session, you are going to learn the relationship between the predictions given by a linear model and the mean estimate given by a conditional Gaussian distribution. We will derive both formula step-by-step and you should implement them in Python.\n",
    "\n",
    "\n",
    "### Dataset creation\n",
    "\n",
    "We are going to create a toy dataset $D=\\{(\\mathbf{x_1},y_1),...(\\mathbf{x_n},y_n)\\}$, where $\\mathbf{x} \\in \\mathbb{R}^3$ and $y \\in \\mathbb{R}$. The $\\mathbf{x}$ is created in the following way:\n",
    "\n",
    "\n",
    "1.   First we sample a number in $[0,1]$ for each dimension\n",
    "2.   Then we multiply each dimension with a random integer in the range $[1,20]$\n",
    "\n",
    "This way, we now have random $x$'s with very different values. Since we want to have a bias term in our linear model, we use the trick of adding a of 1 in front of our $\\mathbf{x} \\in D$.\n",
    "\n",
    "The next step is to generate the $y$ for each $\\mathbf{x}$. We have to define the real weights vector $\\mathbf{w}\\in \\mathbb{R}^4$ and then compute \n",
    "$$y = \\mathbf{w}^T \\mathbf{x} + \\epsilon$$\n",
    "where $\\epsilon$ is the zero-mean and unit variance Gaussian noise.\n",
    "\n",
    "At this point, we have our datset $D$ ready. We will split it in a training set $\\mathbf{X}$ and a test set $\\mathbf{X_*}$ (we here will adhere to a typical setting were 80% of the data goes into the training set and the remaining 20% will be reserved for testing. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "nRNyvzCXVk0O",
    "outputId": "96bee2fb-598b-4cbd-b9de-199925bb7719"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Safety check: sizes\n",
      "(80, 4)\n",
      "(80,)\n",
      "(20, 4)\n",
      "(20,)\n"
     ]
    }
   ],
   "source": [
    "## we have to create our dataset\n",
    "real_weights = np.array([3.24, 1.27, -4.52, 1.75])\n",
    "\n",
    "# number of observations\n",
    "N = 100\n",
    "# dimension of X\n",
    "d = 3\n",
    "X = np.random.rand(N,d) * np.random.randint(1,20,(N,d)) # Nxd matrix\n",
    "\n",
    "# we add the bias term adding a column of 1 in front\n",
    "# of our matrix X\n",
    "X = np.hstack((np.ones((N,1)), X)) # Nx(d+1)\n",
    "\n",
    "# compute the y's for each example\n",
    "# noisy --> add random noise\n",
    "y = real_weights @ X.T + np.random.randn(N)# N\n",
    "\n",
    "# now we can split the dataset in training and test set\n",
    "Xtrain = X[:80,:]\n",
    "ytrain = y[:80]\n",
    "\n",
    "Xtest = X[80:,:]\n",
    "ytest = y[80:]\n",
    "\n",
    "print('Safety check: sizes')\n",
    "print(Xtrain.shape)\n",
    "print(ytrain.shape)\n",
    "print(Xtest.shape)\n",
    "print(ytest.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8zVvrACYr_Fw"
   },
   "source": [
    "### Conditional Gaussian\n",
    "\n",
    "We start by assuming that\n",
    "\n",
    "$$\\begin{bmatrix} y \\\\ \\mathbf{x} \\end{bmatrix} \\sim \\mathcal{N}\\left(\\mathbf{0}, \\begin{bmatrix} \\Sigma_{yy} & \\Sigma_{yx} \\\\ \\Sigma_{xy} & \\Sigma{xx} \\end{bmatrix} \\right) $$\n",
    "\n",
    "where $\\Sigma_{yy} \\in \\mathbb{R}$ and $\\Sigma_{yy} = \\sigma_{y}^2$, i.e. it is the variance of the $y$ because $y \\in \\mathbb{R}$. $\\Sigma_{xx}$ is instead the covariance matrix related to the input vectors $\\mathbf{x}$. \n",
    "\n",
    "<font color='blue'>Tasks:</font>\n",
    "\n",
    "<font color='blue'>**1-** Since we are assuming a zero-mean distribution, we should center or standardize our data. </font>\n",
    "\n",
    "<font color='blue'>**2-** In the distribution above, we are considering the distribution of a vector created by concatenating the training target $y_i$ to each training input $\\mathbf{x}_i$. Have a look at the python function `numpy.hstack` to implement this step.</font>\n",
    "\n",
    "<font color='blue'>**3-** At this point you can compute the sample mean and the sample covariance of this distribution. Be careful and read the documentation of `np.cov`.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "iljs7e25r-Ze",
    "outputId": "9a9f0432-ab1c-4734-8197-f50ff75459d3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Safety check\n",
      "(80, 5)\n"
     ]
    }
   ],
   "source": [
    "# we assume that the traning data (y,x) ~ N(0,\\Sigma)\n",
    "# to have 0 mean we have to center the data\n",
    "\n",
    "## WRITE THE CODE TO CENTER THE Xs\n",
    "Xtrain_mean = np.mean(Xtrain, axis=0)\n",
    "Xtrain = Xtrain - Xtrain_mean\n",
    "#Xtrain[:,1:] = Xtrain[:,1:] - Xtrain_mean[1:] # If we do not want to center the bias-term (intercept)\n",
    "\n",
    "## WRITE THE CODE TO CENTER THE ys\n",
    "ytrain_mean = np.mean(ytrain, axis=0)\n",
    "ytrain = ytrain - ytrain_mean\n",
    "\n",
    "# create arrays [y,x1,x2,...,xd] for each array\n",
    "dataset = np.hstack((np.vstack(ytrain),Xtrain))\n",
    "\n",
    "print('Safety check')# you should get (80, 5) as shape\n",
    "print(dataset.shape)\n",
    "\n",
    "\n",
    "# COMPUTE THE SAMPLE MEAN AND SAMPLE COVARIANCE\n",
    "mu_dataset = np.mean(dataset, axis=0)\n",
    "cov_dataset = np.cov(dataset.T)        # you should get a dxd matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5A7Cl6IcBGIK"
   },
   "source": [
    "At this point we have the distribution of $\\begin{bmatrix} y \\\\ \\mathbf{x} \\end{bmatrix}$ defined above. However, we are interested in the conditional probability $y|\\mathbf{x}$. We know from the lecture that the conditional distribution of a Gaussian distribution is still a Gaussian distribution. \n",
    "\n",
    "Therefore we are looking for:\n",
    "\n",
    "$$ y | \\mathbf{x} \\sim \\mathcal{N}(\\mathbf{\\mu}_{y|\\mathbf{x}}, \\Sigma_{y|\\mathbf{x}})$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\\mathbf{\\mu}_{y|\\mathbf{x}} = \\mu_{y} + \\Sigma_{yx}\\Sigma_{xx}^{-1}(\\mathbf{x}-\\mu_{x}) $$\n",
    "\n",
    "$$\\Sigma_{y|\\mathbf{x}} = \\Sigma_{yy} - \\Sigma_{yx}\\Sigma_{xx}^{-1}\\Sigma_{xy}$$\n",
    "\n",
    "Since we have centered our data, we have that both $\\mu_{y} = 0$ and $\\mu_{x}= 0$. Therefore, the equation for computing the $\\mathbf{\\mu}_{y|\\mathbf{x}}$ becomes easier:\n",
    "\n",
    "$$\\mathbf{\\mu}_{y|\\mathbf{x}} = \\Sigma_{yx}\\Sigma_{xx}^{-1}\\mathbf{x} $$\n",
    "\n",
    "We are mostly interested in the mean of this distribution, because it is the one that give us the mean estimate for every input $\\mathbf{x}$. Indeed. if we look at the definition of the mean we can see that it is a linear function of the inputs $\\mathbf{x}$. For simplicity you can write it as $\\mathbf{\\mu}_{y|\\mathbf{x}} = \\mathbf{c} \\mathbf{x}$, where $\\mathbf{c}=\\Sigma_{yx}\\Sigma_{xx}^{-1}$. As we are going to see later, this is really similar to what we get when we are using a linear model for predictions. \n",
    "\n",
    "\n",
    "<font color='blue'>Tasks:</font>\n",
    "\n",
    "<font color='blue'> **1**- Starting from the mean and covariance you have computed at the previous step, you should compute the vector $\\mathbf{c}=\\Sigma_{yx}\\Sigma_{xx}^{-1}$, because if we know its value, we are able to compute the value of the mean $\\mathbf{\\mu}_{y|\\mathbf{x}}^*$ for every test input vectors $\\mathbf{x}_*$. To compute the inverse of a matrix you can use `np.linalg.pinv`.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "colab_type": "code",
    "id": "WmdtgxqHGbV1",
    "outputId": "54a45870-477d-48d2-f81c-8271eee38687"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Safety check covariance\n",
      "()\n",
      "(4,)\n",
      "(4,)\n",
      "(4, 4)\n",
      "Safety check\n",
      "(4,)\n",
      "The C \n"
     ]
    }
   ],
   "source": [
    "# we are interested in p(y|x1,..xd) which is Gaussian given by\n",
    "# mu_yx = Sigmayx Simgaxx^-1 x\n",
    "\n",
    "# SEPARATE ALL THE PIECES IN THE COVARIANCE MATRIX\n",
    "Sigma_yy = cov_dataset[0,0]\n",
    "Sigma_yx = cov_dataset[1:,0]\n",
    "Sigma_xy = cov_dataset[0,1:]\n",
    "Sigma_xx = cov_dataset[1:,1:]\n",
    "\n",
    "print('Safety check covariance')\n",
    "print(Sigma_yy.shape)\n",
    "print(Sigma_xy.shape)\n",
    "print(Sigma_yx.shape)\n",
    "print(Sigma_xx.shape)\n",
    "\n",
    "## \n",
    "\n",
    "# COMPUTE THE INVERSE AND THEN DO THE MATRIX MULTIPLICATION \\Sigmayx\\Sigmaxx^-1\n",
    "C = Sigma_yx @ np.linalg.pinv(Sigma_xx)\n",
    "\n",
    "print('Safety check')\n",
    "print(C.shape)\n",
    "print('The C ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MrO-2VK-IhwT"
   },
   "source": [
    "### Linear models\n",
    "\n",
    "<font color='grey'>  Notation alert! We are going to write summations in matrix notation, therefore you should be comfortable with using vectors and matrices. </font>\n",
    "\n",
    "\n",
    "Here we are going to derive again the Normal Equation that you have already seen in your Machine Learning course. In regression, we usually try to learn a model of this form:\n",
    "$$y = \\mathbf{w}^T\\mathbf{x}$$\n",
    "where $\\mathbf{w}=[w_0, w_1,\\dotsc,w_D]$ and $\\mathbf{x} = [1, x_0, x_1, \\dotsc, x_D]$.\n",
    "\n",
    "Given a dataset that contains $N$ training examples $(\\mathbf{x}_i,y_i)_{i=1,\\dotsc,N}$, we learn the weigths or parameters of the model $\\mathbf{w}$ by minimizing the Mean-Squared Error of the dataset given by\n",
    "$$ E(\\mathbf{w}) = \\frac{1}{2} \\sum_{n=1}^{N}(y_n - \\mathbf{w}^T\\mathbf{x}_n)^2$$\n",
    "\n",
    "We can rewrite $E(\\mathbf{w})$ as\n",
    "\\begin{align}\n",
    "E(\\mathbf{w}) &= \\frac{1}{2} \\sum_{n=1}^{N}(y_n - \\mathbf{w}^T\\mathbf{x}_n)^2 \\\\\n",
    "              &= \\frac{1}{2} \\left\\| \\mathbf{X}\\mathbf{w} - \\mathbf{y} \\right\\|^2 \\\\\n",
    "              &= \\frac{1}{2} (\\mathbf{X}\\mathbf{w} - \\mathbf{y})^T(\\mathbf{X}\\mathbf{w} - \\mathbf{y}) \\\\\n",
    "              &= \\frac{1}{2} \\mathbf{w}^T\\mathbf{X}^T\\mathbf{X}\\mathbf{w} - \\frac{1}{2}\\mathbf{w}^T\\mathbf{X}^T\\mathbf{y} - \\frac{1}{2} \\mathbf{y}^T\\mathbf{X}\\mathbf{w} + \\frac{1}{2} \\mathbf{y}^T\\mathbf{y} \\\\\n",
    "              &= \\frac{1}{2} \\mathbf{w}^T\\mathbf{X}^T\\mathbf{X}\\mathbf{w} - \\mathbf{w}^T\\mathbf{X}^T\\mathbf{y} + \\frac{1}{2} \\mathbf{y}^T\\mathbf{y}\n",
    "\\end{align}\n",
    "\n",
    "where in the last line we use the fact that $(\\mathbf{X}\\mathbf{w})^T \\mathbf{y} = \\mathbf{y}^T(\\mathbf{X}\\mathbf{w})$ because the inner product is commutative and the fact that $(\\mathbf{X}\\mathbf{w})^T=\\mathbf{w}^T\\mathbf{X}^T$.\n",
    "\n",
    "Since we want to find the weights $\\mathbf{w}$ that minimize $E(\\mathbf{w})$, from calculus we know that to solve this problem we can take the gradient of $E(\\mathbf{w})$, and then set it to zero and solve the resulting system. \n",
    "$$\\nabla E(\\mathbf{w}) = \\begin{bmatrix} \\frac{\\partial E}{\\partial w_0}\\\\ \\frac{\\partial E}{\\partial w_1}\\\\ \\vdots \\\\ \\frac{\\partial E}{\\partial w_D} \\end{bmatrix} = 0$$ \n",
    "\n",
    "Two compute the gradient we use two results that are important to know:\n",
    "$$ f(\\mathbf{w}) = \\mathbf{w}^T\\mathbf{A}\\mathbf{w} \\implies \\nabla f(\\mathbf{w}) = 2 \\mathbf{A}\\mathbf{w}$$\n",
    "$$ f(\\mathbf{w}) = \\mathbf{a}^T\\mathbf{w}  \\implies \\nabla f(\\mathbf{w}) = \\mathbf{a}$$\n",
    "\n",
    "Therefore we can see that the gradient of $E(\\mathbf{w})$ is given by:\n",
    "$$\\nabla E(\\mathbf{w}) = \\mathbf{X}^T \\mathbf{X} \\mathbf{w} - \\mathbf{X}^T\\mathbf{y} $$\n",
    "and if we put this to zero we get\n",
    "\\begin{align}\n",
    "\\mathbf{X}^T \\mathbf{X} \\mathbf{w} - \\mathbf{X}^T\\mathbf{y} = 0 \\\\\n",
    "\\mathbf{X}^T \\mathbf{X} \\mathbf{w} = \\mathbf{X}^T\\mathbf{y} \\\\\n",
    "\\mathbf{w}  = (\\mathbf{X}^T \\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y} \n",
    "\\end{align}\n",
    "\n",
    "<font color='blue'> Tasks:</font>\n",
    "\n",
    "<font color='blue'> **1**- You should code the normal equation in Python. The easiest way to compute it is to write all the matrix multiplication as shown in the formula. Remember, if you want to compute the inverse of a matrix use `numpy.linalg.pinv`. </font>\n",
    "\n",
    "<font color='blue'> (There are other ways you can maybe implement the normal equation, but for this exercise we required you to use the easiest one. You maybe notice that you are trying to solve a linear system, therefore you would try to use `numpy.linalg.solve`. However, in case our system is either over or underconstrained this method will return an error. In those cases you can consider `numpy.linalg.lstsq` which will return the solution that minimizes the squared error.) </font>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "5TUI00zs1FFo",
    "outputId": "e90ade9d-b15d-48fd-bf0c-75a0763e1719"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The weigths obtained by solving the normal equation are: [ 0.          1.26636126 -4.46762863  1.70423914]\n"
     ]
    }
   ],
   "source": [
    "# WRITE THE NORMAL EQUATION\n",
    "\n",
    "w = np.linalg.pinv(Xtrain.T @ Xtrain) @ (Xtrain.T @ ytrain)\n",
    "\n",
    "print('The weigths obtained by solving the normal equation are:', w)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "h2ZTxI8g6aXu"
   },
   "source": [
    "### Predictions\n",
    "\n",
    "You have already seen in previous courses how to compute the prediction $y_*$ for a test example $\\mathbf{x}_*$:\n",
    "$$y_* = \\mathbf{w}^T \\mathbf{x}_*$$\n",
    "using the weights you have computed in the previous step.\n",
    "\n",
    "Maybe you have not seen before that you can use multivariate Gaussian distribution to make predictions. For predicting the target $y_*$ for a particular test input $x_*$ you should use the conditional Gaussian distribution $y_*|\\mathbf{x}_*$, that you have computed above. Indeed, you want to compute the distribution of the possible targets $y_*$ given the test input $x_*$. A good estimate of $y_*$ would be the mean of this distribution, therefore you should compute:\n",
    "$$y_* = \\mu_{y_*|\\mathbf{x}_*} = \\Sigma_{yx}\\Sigma_{xx}^{-1}\\mathbf{x}_*$$\n",
    "where the $\\Sigma_{yx}$ and $\\Sigma_{xx}$ are those you have calculated from the training set.\n",
    "\n",
    "<font color='blue'>Tasks: </font>\n",
    "\n",
    "<font color='blue'> **1**- For each test point, you should compute the prediction given by the linear model and the mena of the conditional distribution, and plot it. What do you notice? \n",
    "\n",
    "**N.B**: Remember that every modifications you did in the training set should be done also in the test set. This means that  you should center each test set (both the input $\\mathbf{x}_*$ and the true target $y$ using the means you have computed in the training set!) </font>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 129
    },
    "colab_type": "code",
    "id": "XIdKUWL6R40y",
    "outputId": "d2a1c737-e61e-418a-888b-52240c78467c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example no.    y (true)    y_hat (lin. reg.)    mu(y|x) (cond. mean)\n",
      "          1       12.88                12.54                   12.54\n",
      "          2       -7.98                -9.02                   -9.02\n",
      "          3      -34.22               -34.70                  -34.70\n",
      "          4       19.27                19.45                   19.45\n",
      "          5       -1.26                -0.49                   -0.49\n",
      "          6      -38.13               -39.66                  -39.66\n",
      "          7       15.70                13.94                   13.94\n",
      "          8       -6.45                -8.91                   -8.91\n",
      "          9       13.65                10.57                   10.57\n",
      "         10        1.72                 1.43                    1.43\n",
      "         11       -1.63                -1.10                   -1.10\n",
      "         12        9.19                 9.32                    9.32\n",
      "         13        2.49                 2.14                    2.14\n",
      "         14       20.84                19.08                   19.08\n",
      "         15        3.24                 2.35                    2.35\n",
      "         16       -7.77                -7.62                   -7.62\n",
      "         17       -2.11                -1.38                   -1.38\n",
      "         18        2.65                 2.21                    2.21\n",
      "         19        2.08                 2.46                    2.46\n",
      "         20       28.25                29.85                   29.85\n"
     ]
    }
   ],
   "source": [
    "# CENTER THE TEST EXAMPLES (by Xtrain mean)\n",
    "Xtest = Xtest - Xtrain_mean\n",
    "ytest = ytest - ytrain_mean\n",
    "\n",
    "# COMPUTE THE PREDICTION USING THE LINEAR MODEL\n",
    "yhat = w @ Xtest.T\n",
    "\n",
    "# COMPUTE THE MEAN GIVEN BY THE CONDITIONAL GAUSSIAN\n",
    "mu_hat = C @ Xtest.T \n",
    "\n",
    "# PRINT THE RESULTS\n",
    "columns = ('Example no.', 'y (true)', 'y_hat (lin. reg.)', 'mu(y|x) (cond. mean)')\n",
    "print('    '.join(columns))\n",
    "for i, (y, yh, ym) in enumerate(zip(ytest, yhat, mu_hat)):\n",
    "    print('{:11d}    {:8.2f}    {:17.2f}    {:20.2f}'.format(i+1, y, yh, ym))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V_ZQzkn_VDMo"
   },
   "source": [
    "### Why do we get this result?\n",
    "\n",
    "<font color='blue'> From the results you get, what can you notice? Try to think at the possible reasons why you observe this findings! </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n583EwNaht9m"
   },
   "source": [
    "We proved that the two methods are equal. Thus the conditional ... is an instance of the genereal linear regression.\n",
    "\n",
    "We can predict using the covariance thereby using a probabilistic approach rather than fitting the weights using gradient optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Exercise1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
